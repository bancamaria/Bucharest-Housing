{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "342_Brancoveanu Anca-Maria_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR1ZlT9j0U4p"
      },
      "source": [
        "# Assignment 1: Bucharest Housing Dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfXso_GoZk9u"
      },
      "source": [
        "## Dataset Description\n",
        "In the dataset linked below you have over three thousand apartments listed for sale on the locally popular website *imobiliare.ro*. Each entry provides details about different aspects of the house or apartment:\n",
        "1. `Nr Camere` indicates the number of rooms;\n",
        "2. `Suprafata` specifies the total area of the dwelling;\n",
        "3. `Etaj` specifies the floor that the home is located at;\n",
        "4. `Total Etaje` is the total number of floors of the block of flats;\n",
        "5. `Sector` represents the administrative district of Bucharest in which the apartment is located;\n",
        "6. `Pret` represents the listing price of each dwelling;\n",
        "7. `Scor` represents a rating between 1 and 5 of location of the apartment. It was computed in the following manner by the dataset creator:\n",
        "  1. The initial dataset included the address of each flat;\n",
        "  2. An extra dataset was used, which included the average sales price of dwellings in different areas of town;\n",
        "  3. Using all of these monthly averages, a clusterization algorithm grouped them into 5 classes, which were then labelled 1-5;\n",
        "  4. You can think of these scores as an indication of the value of the surrounding area, with 1 being expensive, and 5 being inexpensive.\n",
        "\n",
        "Dataset Source: [kaggle.com/denisadutca](https://www.kaggle.com/denisadutca/bucharest-house-price-dataset/kernels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwVnR01-ZmIE"
      },
      "source": [
        "## To Do\n",
        "\n",
        "To complete this assignment, you must:\n",
        "1. Get the data in a PyTorch-friendly format;\n",
        "2. Predict the `Nr Camere` of each dwelling, treating it as a **classification** problem. Choose an appropriate loss function;\n",
        "3. Predict the `Nr Camere` of each dwelling, treating it as a **regression** problem. Choose an appropriate loss function;\n",
        "4. Compare the results of the two approaches, displaying the Confusion Matrix for the two, as well as any comparing any other metrics you think are interesting (e.g. MSE). Comment on the results;\n",
        "5. Choose to predict a feature more suitable to be treated as a **regression** problem, then successfully solve it.\n",
        "6. What values should the loss have when the predictions are random (when your network is not trained at all)?\n",
        "7. Don't forget to split the dataset in training and validation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noDnc4PEZnOk"
      },
      "source": [
        "## Hints\n",
        "1. It might prove useful to link your Google Drive to this Notebook. See the code cell below;\n",
        "2. You might want to think of ways of preprocessing your data (e.g. One Hot Encoding, etc.);\n",
        "3. Don't be afraid of using text cells to actually write your thoughts about the data/results. Might prove useful at the end of the semester when you'll need to walk us through your solution ðŸ˜‰.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pjwrt_IZoeQ"
      },
      "source": [
        "## Deadline\n",
        "March 18, 2021, 23:59\n",
        "\n",
        "**Punctaj maxim:** 2 puncte.\n",
        "\n",
        "Depunctarea este de 0.25 puncte pe zi intarziata. Dupa mai mult de 4 zile intarziere, punctajul maxim care se poate obtine ramane 1 punct.\n",
        "\n",
        "Trimite notebookul si datasetul intr-o arhiva `NumePrenume_Grupa_Tema1.zip` aici: https://forms.gle/MGrLvehEjmtWmQZP7 (la sustinerea temei, vei rula codul din arhiva)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_epOks2gzT3O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b6b1b25-a970-41b5-8cd2-07c0e3308eee"
      },
      "source": [
        "!pip install scikit-learn==0.24\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# Assignment solved with the help of Lab2(solution), PyTorch documentation and the solution posted in the dataset \n",
        "\n",
        "\n",
        "# read data from file\n",
        "df1 = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/BrÃ¢ncoveanuAncaMaria_342_Tema1/Bucharest_HousePriceDataset.csv')\n",
        "df1.dataframeName = 'Bucharest_HousePriceDataset.csv'\n",
        "\n",
        "# In order to use all given data, we must split them into train, validation and test.\n",
        "# Good practice says that we can have 20% of the data reserved for validation and 80% for training\n",
        "\n",
        "x = df1.drop(columns='Nr Camere').to_numpy() \n",
        "y = df1[['Nr Camere']].values.ravel()\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=0.8)\n",
        "\n",
        "# normalise training data\n",
        "std_scale = StandardScaler().fit(x_train)\n",
        "x_train = std_scale.transform(x_train)\n",
        "x_train = torch.tensor(x_train).float()\n",
        "\n",
        "# normalise validation data\n",
        "x_valid = std_scale.transform(x_valid)\n",
        "x_valid = torch.tensor(x_valid).float()\n",
        "\n",
        "# make tensors\n",
        "y_train = torch.Tensor(y_train)\n",
        "y_valid = torch.Tensor(y_valid)\n",
        "\n",
        "\n",
        "###################################################################################################################################\n",
        "######################################### Predict `Nr Camere` as a Classification problem #########################################\n",
        "###################################################################################################################################\n",
        "\n",
        "\n",
        "class multi_layer_perceptron(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_size: int, \n",
        "               hidden_size1: int,\n",
        "               hidden_size2: int,\n",
        "               hidden_size3: int,\n",
        "               hidden_size4: int,\n",
        "               output_size: int):\n",
        "        super().__init__()\n",
        "        self._layer1 = nn.Linear(input_size, hidden_size1)\n",
        "        self._layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self._layer3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self._layer4 = nn.Linear(hidden_size3, hidden_size4)\n",
        "        self._layer5 = nn.Linear(hidden_size4, output_size)\n",
        "  # build multilayer by using rectified linear activation function (ReLU), because it is easier to train a model and it achieves better performance\n",
        "  # if no activation function is used, then it will be the same as the linear regression model \n",
        "  def forward(self, x):\n",
        "        x = torch.relu(self._layer1(x))\n",
        "        x = torch.relu(self._layer2(x))\n",
        "        x = torch.relu(self._layer3(x))\n",
        "        x = torch.relu(self._layer4(x))\n",
        "        x = self._layer5(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "### train model on training data\n",
        "model = multi_layer_perceptron(6,9,9,9,9,9)\n",
        "num_epoch = 500\n",
        "# build an optimizer object that will hold the current state and will update the parameters based on the computed gradients\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for e in range(num_epoch):\n",
        "      # Set the model to train mode and reset the gradients\n",
        "      model.train()\n",
        "      optim.zero_grad()\n",
        "      output = model(x_train)\n",
        "      loss = F.cross_entropy(output, y_train.long() - 1)\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "with torch.no_grad():\n",
        "      y_pred = model(x_train)\n",
        "\n",
        "\n",
        "# training accuracy\n",
        "predicted = torch.argmax(y_pred, dim=-1)\n",
        "accuracy = accuracy_score(y_train - 1, predicted)\n",
        "print(\"\\nTraining accuracy for Classification problem is \", accuracy)\n",
        "\n",
        "with torch.no_grad():\n",
        "      y_pred = model(x_valid)\n",
        "\n",
        "\n",
        "# validation accuracy\n",
        "predicted = torch.argmax(y_pred, dim=-1)\n",
        "class_accuracy = accuracy_score(y_valid - 1, predicted)\n",
        "print(\"Validation accuracy for Classification problem is \", class_accuracy)\n",
        "\n",
        "\n",
        "# display confusion matrix\n",
        "class_matrix = confusion_matrix(y_valid - 1, predicted)\n",
        "class_mse = F.mse_loss(predicted, y_valid - 1).numpy()\n",
        "print(class_matrix)\n",
        "print('Mean Squared Error:', class_mse)\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################################################################\n",
        "######################################### Predict `Nr Camere` as a Regression problem #########################################\n",
        "###############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "class GD_linear_regression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # initializing our model random weights\n",
        "    self.w = nn.Parameter(torch.randn(6, requires_grad = True))\n",
        "    self.b = nn.Parameter(torch.randn(1, requires_grad = True))\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
        "    y = x @ self.w + self.b     # y = wx + b\n",
        "    return y\n",
        "\n",
        "  # PyTorch is accumulating gradients; after each Gradient Descent step we should reset the gradients\n",
        "  def zero_grad(self):\n",
        "    self.w.grad.zero_()\n",
        "    self.b.grad.zero_()\n",
        "\n",
        "\n",
        "### train model on training data\n",
        "model = GD_linear_regression()\n",
        "num_epoch = 500\n",
        "# build an optimizer object that will hold the current state and will update the parameters based on the computed gradients\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for e in range(num_epoch):\n",
        "    # Set the model to train mode and reset the gradients\n",
        "    model.train()\n",
        "    optim.zero_grad()\n",
        "    output = model(x_train)\n",
        "    loss = F.l1_loss(output, y_train)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "with torch.no_grad():\n",
        "      y_pred = model(x_train)\n",
        "\n",
        "\n",
        "# training accuracy\n",
        "predicted = y_pred.round()\n",
        "accuracy = accuracy_score(y_train, predicted)\n",
        "print(\"\\nTraining accuracy for Regression problem is \", accuracy)\n",
        "\n",
        "with torch.no_grad():\n",
        "      y_pred = model(x_valid)\n",
        "\n",
        "\n",
        "# validation accuracy\n",
        "predicted = y_pred.round()\n",
        "regression_accuracy = accuracy_score(y_valid, predicted)\n",
        "print(\"Validation accuracy for Regression problem is \", regression_accuracy)\n",
        "\n",
        "\n",
        "# display confusion matrix\n",
        "regression_matrix = confusion_matrix(y_valid, predicted)\n",
        "regression_mse = F.mse_loss(predicted, y_valid).numpy()\n",
        "print(regression_matrix)\n",
        "print('Mean Squared Error:', regression_mse)\n",
        "\n",
        "\n",
        "# compare results\n",
        "print(\"\\nCompare results:\")\n",
        "print(\"\\n\\t\\tClassification\\t\\tvs\\tRegression\")\n",
        "print(f\"Accuracy:\\t{class_accuracy}\\t\\t{regression_accuracy}\")\n",
        "print(f\"MSE:\\t\\t{class_mse}\\t\\t{regression_mse}\")\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################################################################\n",
        "######################################### Predict `Suprafata` as a Regression problem #########################################\n",
        "###############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "x = df1.drop(columns='Suprafata').to_numpy()\n",
        "y = df1[['Suprafata']].values.ravel()\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=0.8)\n",
        "\n",
        "# normalise training data\n",
        "std_scale = StandardScaler().fit(x_train)\n",
        "x_train = std_scale.transform(x_train)\n",
        "x_train = torch.tensor(x_train).float()\n",
        "\n",
        "# normalise validation data\n",
        "x_valid = std_scale.transform(x_valid)\n",
        "x_valid = torch.tensor(x_valid).float()\n",
        "\n",
        "y_train = torch.Tensor(y_train)\n",
        "y_valid = torch.Tensor(y_valid)\n",
        "\n",
        "model = GD_linear_regression()\n",
        "num_epoch = 300\n",
        "# build an optimizer object that will hold the current state and will update the parameters based on the computed gradients\n",
        "optim = torch.optim.Adam(model.parameters(), lr=3.1)\n",
        "\n",
        "for e in range(num_epoch):\n",
        "    # Set the model to train mode and reset the gradients\n",
        "    model.train()\n",
        "    optim.zero_grad()\n",
        "    output = model(x_train)\n",
        "    loss = F.l1_loss(output, y_train)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "with torch.no_grad():\n",
        "      y_pred = model(x_train)\n",
        "\n",
        "\n",
        "# training accuracy\n",
        "accuracy = 1 - mean_absolute_percentage_error(y_train, y_pred)\n",
        "print(\"\\n\\nTraining accuracy for 2nd Regression problem is \", accuracy)\n",
        "\n",
        "with torch.no_grad():\n",
        "      y_pred = model(x_valid)\n",
        "\n",
        "\n",
        "# validation accuracy\n",
        "regression_accuracy_surface = 1 - mean_absolute_percentage_error(y_valid, y_pred)\n",
        "print(\"Validation accuracy for 2nd Regression problem is \", regression_accuracy_surf)\n",
        "\n",
        "\n",
        "regression_mse_surface = F.mse_loss(predicted, y_valid).numpy()\n",
        "print('Mean Squared Error:', regression_mse_surface)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 6) What values should the loss have when the predictions are random (when your network is not trained at all)?\n",
        "# When the predictions are random, the values shoud be bigger, because the network is not trained\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/ed/ab51a8da34d2b3f4524b21093081e7f9e2ddf1c9eac9f795dcf68ad0a57d/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.3MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Training accuracy for Classification problem is  0.7948990435706695\n",
            "Validation accuracy for Classification problem is  0.8087818696883853\n",
            "[[ 64  16   0   0   0   0   0]\n",
            " [  8 272  27   2   0   0   0]\n",
            " [  0  24 198  14   1   0   0]\n",
            " [  0   1  34  36   0   0   0]\n",
            " [  0   0   0   4   1   0   0]\n",
            " [  0   0   0   1   1   0   0]\n",
            " [  0   0   0   0   2   0   0]]\n",
            "Mean Squared Error: 0.24645892\n",
            "\n",
            "Training accuracy for Regression problem is  0.7088204038257173\n",
            "Validation accuracy for Regression problem is  0.6983002832861189\n",
            "[[ 23  57   0   0   0   0   0   0   0]\n",
            " [  2 288  19   0   0   0   0   0   0]\n",
            " [  0  59 157  16   4   0   1   0   0]\n",
            " [  0   1  38  24   7   1   0   0   0]\n",
            " [  0   0   1   2   1   1   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   1   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0   1]\n",
            " [  0   0   0   0   0   0   0   0   0]]\n",
            "Mean Squared Error: 0.3654391\n",
            "\n",
            "Compare results:\n",
            "\n",
            "\t\tClassification\t\tvs\tRegression\n",
            "Accuracy:\t0.8087818696883853\t\t0.6983002832861189\n",
            "MSE:\t\t0.24645891785621643\t\t0.3654390871524811\n",
            "\n",
            "\n",
            "Training accuracy for 2nd Regression problem is  0.8883194699883461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6e2c0137e3dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;31m# validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0mregression_accuracy_surface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation accuracy for 2nd Regression problem is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_accuracy_surf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'regression_accuracy_surf' is not defined"
          ]
        }
      ]
    }
  ]
}